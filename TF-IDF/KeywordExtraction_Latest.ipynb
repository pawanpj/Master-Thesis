{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import pickle\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import nltk \n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# stemmer = SnowballStemmer(\"german\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install thefuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from thefuzz import fuzz\n",
    "# from thefuzz import process\n",
    "# fuzz.ratio(\"vereinbarkeit von beruf\", \"vereinbarkeit von familie und beruf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/home/pawan/Questionnaire_new_version.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Ground_Truth'][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = ['Nacaps_2018','WeGe_W2','StuMa2020','Studierdenensurvey2016','Absolventen_20092_Haupt','Promopanel_W4','Studienberechtigte_2008.3',\n",
    "'Wissenschaftlerbefragung2016','Promopanel_W3','Sozialerhebung20','WeGe_W3','Promopanel_W5','sid_corona','Promopanel_W2','Absolventen_2013-2','Absolventen_20092_Promotion',\n",
    "'Sozialerhebung21','Absolventen_20092_Mobilität','Sozialerhebung19']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GT_within_text = [['wissenschaftssystem', 'betreuung', 'wissenschaftliche karriere', 'vereinbarkeit von familie und beruf', 'promovierende', 'promotion', 'finanzierung', 'mobilität', 'gesundheit'],\n",
    " ['geflüchtete', 'studienkolleg', 'studienvorbereitung'],\n",
    " ['masterstudium'],\n",
    " ['evaluation', 'qualifikation', 'studiensituation'],\n",
    " [''],\n",
    " ['promotion', 'arbeitsbedingungen','wissenschaftliche aktivitäten','weiterbildung', 'auslandsaufenthalt'],\n",
    " [''],\n",
    " [''],\n",
    " ['promotion', 'arbeitsbedingungen','wissenschaftliche aktivitäten', 'weiterbildung', 'auslandsaufenthalt'],\n",
    " [''],\n",
    " ['geflüchtete', 'studienkolleg', 'studienvorbereitung'],\n",
    " ['promotion', 'arbeitsbedingungen', 'weiterbildung','wissenschaftliche aktivitäten', 'auslandsaufenthalt', 'gesundheit'],\n",
    " ['digitale lehre', 'wohnsituation', 'finanzielle situation', 'studiensituation', 'corona'],\n",
    " ['promotion', 'arbeitsbedingungen', 'weiterbildung', 'auslandsaufenthalt'],\n",
    " [''],\n",
    " [''],\n",
    " [''],\n",
    " [''],\n",
    " ['']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_keywords = []\n",
    "for i in range(len(GT_within_text)):\n",
    "    only_keywords.append([])\n",
    "    for j in range(len(GT_within_text[i])):\n",
    "        if len(GT_within_text[i][j].split(\" \")) == 1:\n",
    "            only_keywords[i].append(GT_within_text[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "only_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_keyphrases = []\n",
    "for i in range(len(GT_within_text)):\n",
    "    only_keyphrases.append([])\n",
    "    for j in range(len(GT_within_text[i])):\n",
    "        if len(GT_within_text[i][j].split(\" \")) > 1:\n",
    "            only_keyphrases[i].append(GT_within_text[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_keyphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_not_in_text = [['promotionsformen', 'promotionsmotive',  'monetäre erträge', 'nicht-monetäre erträge', 'wissenschaftlicher nachwuchs','promotionsabbruch', 'strukturierte promotion', 'persönlichkeit', 'erwerbsverläufe', 'promovierte'],\n",
    "                            ['integration', 'migration'], \n",
    "                            ['beruflicher verbleib von exmatrikulierten', 'studiensituation', 'studienabbruch', 'abbruchursachen'],\n",
    "                            ['effizienz, zeitreihen', 'studierendenbefragung', 'hochschulforschung'], \n",
    "                            ['hochschulforschung'], \n",
    "                            ['hochschulforschung'],\n",
    "                            ['studienberechtigte', 'hochschulforschung'], \n",
    "                            ['hochschulforschung'], \n",
    "                            ['hochschulforschung'],\n",
    "                            ['hochschulforschung'],\n",
    "                            ['integration', 'migration'], \n",
    "                            ['hochschulforschung'], \n",
    "                            ['internationale studierende', 'finanzielle situation', 'studienerfolg', 'erwerbstätige studierende', 'persönlichkeit', 'beeinträchtigt studierende', 'studierende', 'gesundheit', 'studierendenforschung', 'hochschulforschung'],\n",
    "                            ['hochschulforschung'],\n",
    "                            ['absolventen', 'hochschulforschung'], \n",
    "                            ['hochschulforschung'], \n",
    "                            ['hochschulforschung'], \n",
    "                            ['hochschulforschung'], \n",
    "                            ['hochschulforschung']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ground_truth_not_in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Valid_Ground_Truth'] = GT_within_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Ground_Truth_Not_In_Text'] = ground_truth_not_in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_list3 = [\"/home/pawan/website_keywords_also_exist_within_thesaurus/\" + elem + \".txt\" for elem in file_name]                  \n",
    "# GT_within_thesaurus = []\n",
    "\n",
    "# for file_path in file_list3:\n",
    "#     with open(file_path) as f_input:\n",
    "#         GT_within_thesaurus.append(f_input.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GT_within_thesaurus_not_in_text = ['Promotion, persönlichkeit', 'integration,migration', 'studiensituation, studienabbruch', 'Effizienz, zeitreihen, hochschulforschung',\n",
    "                                  'hochschulforschung','hochschulforschung', 'hochschulforschung', 'hochschulforschung', 'hochschulforschung', 'hochschulforschung',\n",
    "                                   'integration,migration', 'hochschulforschung', 'finanzielle Situation, Studienerfolg, Persönlichkeit, Gesundheit Hochschulforschung', \n",
    "                                  'Hochschulforschung', 'Hochschulforschung', 'hochschulforschung', 'hochschulforschung', 'hochschulforschung','hochschulforschung']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GT_within_thesaurus_not_in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "232\n",
      "890\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('german')\n",
    "stop_words\n",
    "print(len(stop_words))\n",
    "stop_words.extend([\"a\",\"ab\",\"können\",'hs', \"bzw\",'abt','fuer','kuenste', 'bw','tu','kath',  \"usw\",\"eu\",\"wiwi\",\"soz\", \"nah\",\"dsh\",\"and\",\"eng\",\"wahr\", \"kfz\", \"kiel\", \"öl\",\"ca\", \"fil\", \"bmbf\", \"monat\",\"zofar\", \"sose\", \"ha\",\"wi\", \"übt\",\"wis\",\"vwl\",\"geben\", \"hhu\",\"bitte\",\"inkl\", \"läuft\", \"ggf\", \"ern\", \"te\", \"edv\", \"un\",\"ver\", \"finno\", \"etc\",\"ehe\",\"kfw\", \"maße\", \"möchten\", \"cau\", \"juni\", \"uds\", \"innen\", \"aber\",\"ach\",\"acht\",\"achte\",\"achten\",\"achter\",\"achtes\",\"ag\",\"alle\",\"fh\",\"allein\",\"allem\",\"allen\",\"aller\",\"allerdings\",\"alles\",\"allgemeinen\",\"als\",\"also\",\"am\",\"an\",\"andere\",\"anderen\",\"andern\",\"anders\",\"au\",\"auch\",\"auf\",\"aus\",\"ausser\",\"außer\",\"ausserdem\",\"außerdem\",\"b\",\"bald\",\"bei\",\"beide\",\"beiden\",\"beim\",\"beispiel\",\"bekannt\",\"bereits\",\"besonders\",\"besser\",\"üben\",\"besten\",\"bin\",\"sein\",\"können\",\"bis\",\"bisher\",\"bist\",\"c\",\"d\",\"da\",\"dabei\",\"dadurch\",\"dafür\",\"dagegen\",\"daher\",\"dahin\",\"dahinter\",\"damals\",\"damit\",\"danach\",\"daneben\",\"dank\",\"dann\",\"daran\",\"darauf\",\"daraus\",\"darf\",\"darfst\",\"darin\",\"darüber\",\"darum\",\"darunter\",\"das\",\"dasein\",\"daselbst\",\"dass\",\"daß\",\"dasselbe\",\"davon\",\"davor\",\"dazu\",\"dazwischen\",\"dein\",\"deine\",\"deinem\",\"deiner\",\"dem\",\"dementsprechend\",\"demgegenüber\",\"demgemäss\",\"demgemäß\",\"demselben\",\"demzufolge\",\"den\",\"denen\",\"denn\",\"denselben\",\"saarlandes\",\"der\",\"deren\",\"derjenige\",\"derjenigen\",\"dermassen\",\"dermaßen\",\"derselbe\",\"derselben\",\"des\",\"deshalb\",\"desselben\",\"dessen\",\"deswegen\",\"d.h\",\"dich\",\"die\",\"diejenige\",\"diejenigen\",\"dies\",\"diese\",\"dieselbe\",\"dieselben\",\"diesem\",\"diesen\",\"dieser\",\"dieses\",\"dir\",\"doch\",\"dort\",\"drei\",\"drin\",\"dritte\",\"dritten\",\"dritter\",\"drittes\",\"du\",\"durch\",\"durchaus\",\"dürfen\",\"dürft\",\"durfte\",\"durften\",\"e\",\"eben\",\"ebenso\",\"ehrlich\",\"ei\",\"ei,\",\"eigen\",\"eigene\",\"eigenen\",\"eigener\",\"eigenes\",\"ein\",\"einander\",\"eine\",\"einem\",\"einen\",\"einer\",\"eines\",\"einige\",\"einigen\",\"einiger\",\"einiges\",\"einmal\",\"eins\",\"elf\",\"en\",\"ende\",\"endlich\",\"entweder\",\"er\",\"Ernst\",\"erst\",\"erste\",\"ersten\",\"erster\",\"erstes\",\"es\",\"etwa\",\"etwas\",\"euch\",\"f\",\"früher\",\"fünf\",\"fünfte\",\"fünften\",\"fünfter\",\"fünftes\",\"für\",\"g\",\"gab\",\"ganz\",\"ganze\",\"ganzen\",\"ganzer\",\"ganzes\",\"gar\",\"gedurft\",\"gegen\",\"gegenüber\",\"gehabt\",\"gehen\",\"geht\",\"gekannt\",\"gekonnt\",\"gemacht\",\"gemocht\",\"gemusst\",\"genug\",\"gerade\",\"gern\",\"gesagt\",\"geschweige\",\"gewesen\",\"gewollt\",\"geworden\",\"gibt\",\"ging\",\"gleich\",\"gott\",\"gross\",\"groß\",\"grosse\",\"große\",\"grossen\",\"großen\",\"grosser\",\"großer\",\"grosses\",\"großes\",\"gut\",\"gute\",\"guter\",\"gutes\",\"h\",\"habe\",\"haben\",\"habt\",\"hast\",\"hat\",\"hatte\",\"hätte\",\"hatten\",\"hätten\",\"heisst\",\"her\",\"heute\",\"hier\",\"hin\",\"hinter\",\"hoch\",\"i\",\"ich\",\"ihm\",\"ihn\",\"ihnen\",\"ihr\",\"ihre\",\"ihrem\",\"ihren\",\"ihrer\",\"ihres\",\"im\",\"immer\",\"in\",\"indem\",\"infolgedessen\",\"ins\",\"irgend\",\"ist\",\"j\",\"ja\",\"jahr\",\"jahre\",\"jahren\",\"je\",\"jede\",\"jedem\",\"jeden\",\"jeder\",\"jedermann\",\"jedermanns\",\"jedoch\",\"jemand\",\"jemandem\",\"jemanden\",\"jene\",\"jenem\",\"jenen\",\"jener\",\"jenes\",\"jetzt\",\"k\",\"kam\",\"kann\",\"kannst\",\"kaum\",\"kein\",\"keine\",\"keinem\",\"keinen\",\"keiner\",\"kleine\",\"kleinen\",\"kleiner\",\"kleines\",\"kommen\",\"kommt\",\"können\",\"könnt\",\"konnte\",\"könnte\",\"konnten\",\"kurz\",\"l\",\n",
    "                   \"lang\",\"lange\",\"leicht\",\"leide\",\"lieber\",\"los\",\"m\",\"inn\",\"usw \",\"dfg\",\"machen\", \"erc\",\"macht\",\"machte\",\"mag\",\"magst\",\"mahn\",\"man\",\"welch\", \"manche\",\"manchem\",\"manchen\",\"mancher\",\"manches\",\"mann\",\"mehr\",\"mein\",\"meine\",\"meinem\",\"meinen\",\"meiner\",\"meines\",\"mensch\",\"menschen\",\"mich\",\"mir\",\"mit\",\"mittel\",\"mochte\",\"möchte\",\"mochten\",\"düsseldorf\",\"mögen\",\"möglich\",\"mögt\",\"morgen\",\"muss\",\"muß\",\"müssen\",\"musst\",\"müsst\",\"musste\",\"mussten\",\"n\",\"na\",\"nach\",\"pogs\",\"vater\",\"mutter\",\"nachdem\",\"nahm\",\"natürlich\",\"neben\",\"nein\",\"neue\",\"neuen\",\"neun\",\"neunte\",\"neunten\",\"neunter\",\"neuntes\",\"nicht\",\"nichts\",\"nie\",\"niemand\",\"niemandem\",\"niemanden\",\"noch\",\"nun\",\"nur\",\"o\",\"ob\",\"oben\",\"oder\",\"offen\",\"oft\",\"ohne\",\"Ordnung\",\"p\",\"q\",\"r\",\"recht\",\"rechte\",\"rechten\",\"rechter\",\"rechtes\",\"richtig\",\"rund\",\"s\",\"sa\",\"sache\",\"sagt\",\"sagte\",\"sah\",\"satt\",\"schlecht\",\"Schluss\",\"schon\",\"sechs\",\"sechste\",\"sechsten\",\"sechster\",\"sechstes\",\"sehr\",\"sei\",\"seid\",\"seien\",\"sein\",\"seine\",\"seinem\",\"seinen\",\"seiner\",\"seines\",\"seit\",\"seitdem\",\"selbst\",\"sich\",\"sie\",\"sieben\",\"siebente\",\"siebenten\",\"siebenter\",\"siebentes\",\"sind\",\"so\",\"solang\",\"solche\",\"solchem\",\"solchen\",\"solcher\",\"solches\",\"soll\",\"sollen\",\"sollte\",\"sollten\",\"sondern\",\"sonst\",\"sowie\",\"später\",\"statt\",\"t\",\"tag\",\"tage\",\"tagen\",\"tat\",\"teil\",\"tel\",\"tritt\",\"trotzdem\",\"tun\",\"u\",\"über\",\"überhaupt\",\"übrigens\",\"uhr\",\"um\",\"und\",\"und?\",\"uns\",\"unser\",\"unsere\",\"unserer\",\"unter\",\"v\",\"vergangenen\",\"viel\",\"viele\",\"vielem\",\"vielen\",\"vielleicht\",\"vier\",\"vierte\",\"vierten\",\"vierter\",\"viertes\",\"vom\",\"vor\",\"w\",\"wahr?\",\"während\",\"währenddem\",\"währenddessen\",\"wann\",\"war\",\"wäre\",\"waren\",\"wart\",\"warum\",\"was\",\"wegen\",\"weil\",\"weit\",\"weiter\",\"weitere\",\"weiteren\",\"weiteres\",\"welche\",\"welchem\",\"welchen\",\"welcher\",\"welches\",\"wem\",\"wen\",\"wenig\",\"wenige\",\"weniger\",\"weniges\",\"wenigstens\",\"wenn\",\"wer\",\"werde\",\"werden\",\"werdet\",\"wessen\",\"wie\",\"wieder\",\"will\",\"willst\",\"wir\",\"wird\",\"wirklich\",\"wirst\",\"wo\",\"wohl\",\"wollen\",\"wollt\",\"wollte\",\"wollten\",\"worden\",\"wurde\",\"würde\",\"wurden\",\"würden\",\"x\",\"y\",\"z\",\"z.b\",\"zehn\",\"zehnte\",\"zehnten\",\"zehnter\",\"zehntes\",\"zeit\",\"zu\",\"zuerst\",\"zugleich\",\"zum\",\"zunächst\",\"zur\",\"zurück\",\"zusammen\",\"zwanzig\",\"zwar\",\"zwei\",\"zweite\",\"zweiten\",\"zweiter\",\"zweites\",\"zwischen\",\"zwölf\",\"euer\",\"eure\",\"hattest\",\"hattet\",\"jedes\",\"mußt\",\"müßt\",\"sollst\",\"sollt\",\"soweit\",\"weshalb\",\"wieso\",\"woher\",\"wohin\"])\n",
    "print(len(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean_text(text, for_embedding=False):\n",
    "#     \"\"\"\n",
    "#         - remove any html tags (< /br> often found)\n",
    "#         - Keep only ASCII + European Chars and whitespace, no digits\n",
    "#         - remove single letter chars\n",
    "#         - convert all whitespaces (tabs etc.) to single wspace\n",
    "#         if not for embedding (but e.g. tdf-idf):\n",
    "#         - all lowercase\n",
    "#         - remove stopwords, punctuation and stemm\n",
    "#     \"\"\"\n",
    "#     RE_WSPACE = re.compile(r\"\\s+\", re.IGNORECASE)\n",
    "#     RE_TAGS = re.compile(r\"<[^>]+>\")\n",
    "#     RE_ASCII = re.compile(r\"[^A-Za-zÀ-ž ]\", re.IGNORECASE)\n",
    "#     RE_SINGLECHAR = re.compile(r\"\\b[A-Za-zÀ-ž]\\b\", re.IGNORECASE)\n",
    "# #     if for_embedding:\n",
    "#         # Keep punctuation\n",
    "# #         RE_ASCII = re.compile(r\"[^A-Za-zÀ-ž,.!? ]\", re.IGNORECASE)\n",
    "# #         RE_SINGLECHAR = re.compile(r\"\\b[A-Za-zÀ-ž,.!?]\\b\", re.IGNORECASE)\n",
    "\n",
    "#     text = re.sub(RE_TAGS, \" \", text)\n",
    "#     text = re.sub(RE_ASCII, \" \", text)\n",
    "#     text = re.sub(RE_SINGLECHAR, \" \", text)\n",
    "#     text = re.sub(RE_WSPACE, \" \", text)\n",
    "\n",
    "#     word_tokens = word_tokenize(text)\n",
    "#     words_tokens_lower = [word.lower() for word in word_tokens]\n",
    "\n",
    "#     if for_embedding:\n",
    "#         # no stemming, lowering and punctuation / stop words removal\n",
    "#         words_filtered = word_tokens\n",
    "#     else:\n",
    "#         words_filtered = [\n",
    "#             word for word in words_tokens_lower if word not in stop_words\n",
    "#         ] \n",
    "\n",
    "#     text_clean = \" \".join(words_filtered)\n",
    "#     return text_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, for_embedding=False):\n",
    "    \"\"\"\n",
    "        - remove any html tags (< /br> often found)\n",
    "        - Keep only ASCII + European Chars and whitespace, no digits\n",
    "        - remove single letter chars\n",
    "        - convert all whitespaces (tabs etc.) to single wspace\n",
    "        if not for embedding (but e.g. tdf-idf):\n",
    "        - all lowercase\n",
    "        - remove stopwords, punctuation and stemm\n",
    "    \"\"\"\n",
    "    RE_WSPACE = re.compile(r\"\\s+\", re.IGNORECASE)\n",
    "    RE_TAGS = re.compile(r\"<[^>]+>\")\n",
    "    RE_ASCII = re.compile(r\"[^A-Za-zÀ-ž ]\", re.IGNORECASE)\n",
    "    RE_SINGLECHAR = re.compile(r\"\\b[A-Za-zÀ-ž]\\b\", re.IGNORECASE)\n",
    "    if for_embedding:\n",
    "         #Keep punctuation\n",
    "        RE_ASCII = re.compile(r\"[^A-Za-zÀ-ž.! ]\", re.IGNORECASE)\n",
    "        RE_SINGLECHAR = re.compile(r\"\\b[A-Za-zÀ-ž.!?]\\b\", re.IGNORECASE)\n",
    "\n",
    "    text = re.sub(RE_TAGS, \" \", text)\n",
    "    text = re.sub(RE_ASCII, \" \", text)\n",
    "    text = re.sub(RE_SINGLECHAR, \" \", text)\n",
    "    text = re.sub(RE_WSPACE, \" \", text)\n",
    "\n",
    "    word_tokens = word_tokenize(text)\n",
    "    words_tokens_lower = [word.lower() for word in word_tokens]\n",
    "\n",
    "    if for_embedding:\n",
    "        # no stemming, lowering and punctuation / stop words removal\n",
    "        words_filtered = word_tokens\n",
    "    else:\n",
    "        words_filtered = [\n",
    "            word for word in words_tokens_lower if word not in stop_words\n",
    "        ] \n",
    "\n",
    "    text_clean = \" \".join(words_filtered)\n",
    "    return text_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"clean_text\"] = data[\"Text\"].map(\n",
    "    lambda x: clean_text(x, for_embedding=True) if isinstance(x, str) else x\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"clean_text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Preprocesed_text = []\n",
    "for i in range(len(data[\"clean_text\"])):\n",
    "    Preprocesed_text.append(re.sub(r'\\s([?.!\"](?:\\s|$))', r'\\1', data[\"clean_text\"][i]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "test_text = []\n",
    "for i in range(len(Preprocesed_text)):\n",
    "    test_text.append(re.sub(r'\\.+ ', \".\",  Preprocesed_text[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text = []\n",
    "for i in range(len(test_text)):\n",
    "    for line in test_text[i].split('\\n'):\n",
    "        # Replace multiple dots with space\n",
    "        line = re.sub('\\.\\.+', '.', line) \n",
    "        # Remove single dots\n",
    "        cleaned_text.append(re.sub('\\.', '.', line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_cleaned_text = []\n",
    "for i in range(len(cleaned_text)):\n",
    "    final_cleaned_text.append(re.sub(r\"\\.(?=\\S)\", \". \", cleaned_text[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['final_cleaned_text'] = final_cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['final_cleaned_text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"clean_text_wo_sw\"] = data[\"Text\"].map(\n",
    "    lambda x: clean_text(x, for_embedding=False) if isinstance(x, str) else x\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"clean_text_wo_sw\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['final_cleaned_text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# nlp = spacy.load('de_core_news_md')\n",
    "\n",
    "# mails=['In welchem Bundesland haben Sie Ihre Hochschulzugangsberechtigung erworben']\n",
    "\n",
    "# mails_lemma = []\n",
    "\n",
    "# for mail in mails:\n",
    "#      doc = nlp(mail)\n",
    "#      result = ' '.join([x.lemma_ for x in doc]) \n",
    "#      mails_lemma.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('de_core_news_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using spacy model for POS tagging\n",
    "\n",
    "Extracted_tags=[]\n",
    "for i in range(len(data['final_cleaned_text'])):\n",
    "    Extracted_tags.append([])\n",
    "    doc = nlp(data['final_cleaned_text'][i])\n",
    "    for t in doc:\n",
    "        tag=t.pos_\n",
    "        if tag: # for Noun, only \"NOUN\"\n",
    "            if t.text not in Extracted_tags:\n",
    "                Extracted_tags[i].append((t.text, t.pos_))  #for appending to it as a tuples: constructing a tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Extracted_tags[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from germalemma import GermaLemma\n",
    "lemmatizer = GermaLemma()\n",
    "lemmatized_words = []\n",
    "nlp = spacy.load('de_core_news_lg')\n",
    "\n",
    "for i in range(len(Extracted_tags)):\n",
    "    lemmatized_words.append([])\n",
    "    lemma_spacy = nlp(final_cleaned_text[i])\n",
    "    for j in range(len(lemma_spacy)):\n",
    "        token = lemma_spacy[j]\n",
    "        if token.pos_ not in ['NOUN','ADJ', 'VERB','ADV']:\n",
    "            lemmatized_words[i].append((token.lemma_))\n",
    "        else:\n",
    "            #pass\n",
    "            temp = Extracted_tags[i][j][1]\n",
    "            lemmatized_words[i].append(lemmatizer.find_lemma(Extracted_tags[i][j][0], Extracted_tags[i][j][1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the list of tokens into string\n",
    "lemmatized_corpus = [' '.join(x) for x in lemmatized_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding column to the dataframe\n",
    "\n",
    "data['clean_text_w_lemma'] = lemmatized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatization on the corpus text which contains all the stopwords.\n",
    "\n",
    "#spacy model does not have a good lemmatizer as the accuracy of the model for lemmatization is 73%\n",
    "\n",
    "# clean_text_w_lemma_w_sw = []  # clean text with lemmatization\n",
    "\n",
    "# for words in data[\"clean_text\"]:\n",
    "#      doc = nlp(words)\n",
    "#      result = ' '.join([token.lemma_ for token in doc]) \n",
    "#      clean_text_w_lemma_w_sw.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = (data['clean_text']).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data_wo_sw = (data['clean_text_wo_sw']).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data_w_lemma= data['clean_text_w_lemma'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df =data[\"clean_text_wo_sw\"].str.split(expand=True).stack().value_counts().reset_index()\n",
    " \n",
    "new_df.columns = ['Word', 'Frequency'] \n",
    " \n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_df[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import hist\n",
    "\n",
    "hist(new_df[0:5].Word, weights=new_df[0:5].Frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating Document Frequency\n",
    "\n",
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "DF = {}\n",
    "for i in range(len(data['clean_text'])):\n",
    "    tokens = nltk.word_tokenize(data['clean_text'][i]) #without nltk.word_tokenize, it gives character level DF\n",
    "    for w in tokens:\n",
    "            try:\n",
    "                DF[w].add(i)\n",
    "            except:\n",
    "                DF[w] = {i}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(DF) #Total Unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word,freq in DF.items():\n",
    "    print(word,len(freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list=[]\n",
    "for word,freq in DF.items():\n",
    "    df_list.append(tuple((word,len(freq)/19)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list.sort(key=lambda x:x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = [x[1] for x in df_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "df_analysis = pd.DataFrame.from_dict(Counter(word_counts), orient='index').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plot\n",
    "# Draw a vertical bar chart\n",
    "\n",
    "df_analysis.plot.bar(x=\"index\", y=0, rot=65, title=\"Document Frequency Chart\");\n",
    "\n",
    "plot.show(block=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "cv=CountVectorizer(stop_words=stop_words,ngram_range=(1,1)) # min_df =0.05263 # trying different things\n",
    "\n",
    "word_count_vector=cv.fit_transform(sample_data_w_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the numbers are not counts, they are the position in the sparse vector.\n",
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_vector.shape\n",
    "#We have 19 (rows) documents and 3003 unique words (columns)!\n",
    "# With stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.stop_words #gives you the stop words that CountVectorizer inferred from your min_df and max_df settings as well as those that were cut off during feature selection (through the use of max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "\n",
    "tfidf_transformer.fit(word_count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names=cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_names.index('mobilität')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sort_coo(coo_matrix):\n",
    "#     tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "#     return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    "\n",
    "# def extract_topn_from_vector(feature_names, sorted_items, topn):\n",
    "#     \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
    "    \n",
    "#     #use only topn items from vector\n",
    "#     sorted_items = sorted_items[:topn]\n",
    "\n",
    "#     score_vals = []\n",
    "#     feature_vals = []\n",
    "    \n",
    "#     # word index and corresponding tf-idf score\n",
    "#     for idx, score in sorted_items:\n",
    "        \n",
    "#         #keep track of feature name and its corresponding score\n",
    "#         score_vals.append(round(score, 3))\n",
    "#         feature_vals.append(feature_names[idx])\n",
    "\n",
    "#     #create a tuples of feature,score\n",
    "#     #results = zip(feature_vals,score_vals)\n",
    "#     results= {}\n",
    "#     for idx in range(len(feature_vals)):\n",
    "#         results[feature_vals[idx]]=score_vals[idx]\n",
    "    \n",
    "#     return results\n",
    "\n",
    "# def get_keywords(vectorizer, feature_names, doc):\n",
    "#     \"\"\"Return top k keywords from a doc using TF-IDF method\"\"\"\n",
    "\n",
    "#     #generate tf-idf for the given document\n",
    "#     tf_idf_vector = vectorizer.transform(cv.transform([doc]))\n",
    "    \n",
    "#     #sort the tf-idf vectors by descending order of scores\n",
    "#     sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "\n",
    "#     #extract only TOP_K_KEYWORDS\n",
    "#     keywords=extract_topn_from_vector(feature_names,sorted_items,400)\n",
    "    \n",
    "#     return (keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    "\n",
    "def extract_topn_from_vector(feature_names, sorted_items):\n",
    "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
    "    \n",
    "    #use only topn items from vector\n",
    "    sorted_items = sorted_items\n",
    "\n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "    \n",
    "    # word index and corresponding tf-idf score\n",
    "    for idx, score in sorted_items:\n",
    "        \n",
    "        #keep track of feature name and its corresponding score\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    "\n",
    "    #create a tuples of feature,score\n",
    "    #results = zip(feature_vals,score_vals)\n",
    "    results= {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "    \n",
    "    return results\n",
    "\n",
    "def get_keywords(vectorizer, feature_names, doc):\n",
    "    \"\"\"Return top k keywords from a doc using TF-IDF method\"\"\"\n",
    "\n",
    "    #generate tf-idf for the given document\n",
    "    tf_idf_vector = vectorizer.transform(cv.transform([doc]))\n",
    "    \n",
    "    #sort the tf-idf vectors by descending order of scores\n",
    "    sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "\n",
    "    #extract only TOP_K_KEYWORDS\n",
    "    keywords=extract_topn_from_vector(feature_names,sorted_items)\n",
    "    \n",
    "    return (keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = []\n",
    "for doc in sample_data_w_lemma:\n",
    "    df = {}\n",
    "    df['Text'] = doc\n",
    "    df['top_keywords'] = get_keywords(tfidf_transformer, feature_names, doc)\n",
    "    result.append(df)\n",
    "    \n",
    "final = pd.DataFrame(result)\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final['top_keywords'][0]) # document 8 have 189 keywords(<topk(200)) and document 14 have 187 keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final['top_keywords'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final['Valid_Ground_Truth']= data ['Valid_Ground_Truth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for uniqueness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Calculating Document Frequency\n",
    "\n",
    "# from collections import defaultdict\n",
    "# import math\n",
    "\n",
    "# DF2 = {}\n",
    "# for i in range(len(final)):\n",
    "#     for elem in list((final['top_keywords'])[i].keys()):\n",
    "#         tokens = nltk.word_tokenize(elem) #without nltk.word_tokenize, it gives character level DF\n",
    "#         for w in tokens:\n",
    "#             try:\n",
    "#                 DF2[w].add(i)\n",
    "#             except:\n",
    "#                 DF2[w] = {i}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(DF2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for word,freq in DF2.items():\n",
    "#     print(word,len(freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_list1=[]\n",
    "# for word,freq in DF2.items():\n",
    "#     df_list1.append(tuple((word,len(freq)/19)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for uniquenees we can take the amount(count/Frequency) of documents that a keyword occurs in and then dividing it by the total amount of documents.(Document frequency)\n",
    "\n",
    "# something that comes in 19 out of 19 questionnaire, we can be sure that it is not unique and something that comes in 1 out of 19 it’s very unique.\n",
    "\n",
    "# we can define a threshold on document frquency and based on that can define the uniqueness/decisiveness of keyword        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final['Ground_Truth']= data['Ground_Truth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted=[]\n",
    "chunk_size = 1 #coz max ground truth size is 20\n",
    "for i in range(len(final['top_keywords'])):\n",
    "    predicted.append([])           #used for nested list\n",
    "    for j in range(1,int(len(final['top_keywords'][i])/chunk_size)+1):\n",
    "        predicted[i].append(list(final['top_keywords'][i].keys())[0:j*chunk_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted[0]   #predicted[0][-1] shows all the keywords in document 0, that is 200 keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "# function to evaluate success of keyword extraction #\n",
    "######################################################\n",
    "def evaluate_keywords(proposed,only_keywords):\n",
    "  \"\"\"\n",
    "  Returns precision, recall, and f1 score for proposed keywords against ground truth\n",
    "  \"\"\"\n",
    "  proposed_set = set(proposed)\n",
    "  true_set = set(only_keywords)\n",
    "  \n",
    "  true_positives = len(proposed_set.intersection(true_set))\n",
    "  if len(proposed_set)==0:\n",
    "    precision = 0\n",
    "  else:\n",
    "    # note denominator reflects total number of words\n",
    "    # not total number of unique words\n",
    "    precision = true_positives/float(len(proposed)) \n",
    "      \n",
    "  if len(true_set)==0:\n",
    "    recall = 0\n",
    "  else:\n",
    "    recall = true_positives/float(len(true_set))\n",
    "  \n",
    "  if precision + recall > 0:\n",
    "    f1 = 2*precision*recall/float(precision + recall)\n",
    "  else:\n",
    "    f1 = 0\n",
    "\n",
    "  return (precision, recall, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_values = []\n",
    "for i in range(len(predicted)):\n",
    "    metric_values.append([]) \n",
    "    for j in range(len(predicted[i])):\n",
    "        metric_values[i].append(evaluate_keywords(predicted[i][j], only_keywords[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precison_values = []\n",
    "\n",
    "for i in range(len(metric_values)):\n",
    "    precison_values.append([])\n",
    "    for a_tuple in metric_values[i]:\n",
    "        precison_values[i].append(a_tuple[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precison_values[10].index(max(precison_values[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precison_values[10][37]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precison_values[10][33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_values = []\n",
    "for i in range(len(metric_values)):\n",
    "    recall_values.append([])\n",
    "    for a_tuple in metric_values[i]:\n",
    "        recall_values[i].append(a_tuple[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_values[10][37]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_values[10][69]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score = []\n",
    "for i in range(len(metric_values)):\n",
    "    f1_score.append([])\n",
    "    for a_tuple in metric_values[i]:\n",
    "        f1_score[i].append(a_tuple[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_keywords[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "\n",
    "for i in range(len(precison_values)):\n",
    "        print(\"Graph for document \"+ str(i))\n",
    "    \n",
    "        # Create some mock data\n",
    "        t = [x for x in range(1,(len(precison_values[i])+1))]\n",
    "        data1 = precison_values[i]\n",
    "        data2 = recall_values[i]\n",
    "\n",
    "        fig, ax1 = plt.subplots()\n",
    "\n",
    "        color = 'tab:red'\n",
    "        ax1.set_xlabel('Threshold (k)')\n",
    "        ax1.set_ylabel('Precision', color=color)\n",
    "        ax1.plot(t, data1, color=color)\n",
    "        ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "        ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "        color = 'tab:blue'\n",
    "        ax2.set_ylabel('Recall', color=color)  # we already handled the x-label with ax1\n",
    "        ax2.plot(t, data2, color=color)\n",
    "        ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "        fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "        fig.patch.set_visible(False)\n",
    "#         plt.savefig(str(i) + 'Document.png',  dpi=200)  #to save the plots\n",
    "        plt.show()\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final['top_keywords'][10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data['file_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(len(precison_values)):\n",
    "    print(\"Precision Score Graph for Documents \" + str(i))\n",
    "    t = [x for x in range(1,(len(precison_values[i])+1))]\n",
    "    data = recall_values[i]\n",
    "#     fig, ax1 = plt.subplots()\n",
    "\n",
    "    color = 'tab:red'\n",
    "    plt.xlabel(\"Threshold(k)\")\n",
    "    plt.ylabel(\"Precision-Score\")\n",
    "    plt.plot(t, data, color=color)\n",
    "    plt.tick_params(axis='y', labelcolor=color)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(len(recall_values)):\n",
    "    print(\"Recall Score Graph for Documents \" + str(i))\n",
    "    t = [x for x in range(1,(len(recall_values[i])+1))]\n",
    "    data = recall_values[i]\n",
    "#     fig, ax1 = plt.subplots()\n",
    "\n",
    "    color = 'tab:red'\n",
    "    plt.xlabel(\"Threshold(k)\")\n",
    "    plt.ylabel(\"Recall-Score\")\n",
    "    plt.plot(t, data, color=color)\n",
    "    plt.tick_params(axis='y', labelcolor=color)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(len(f1_score)):\n",
    "    print(\"F1 Score Graph for Documents \" + str(i))\n",
    "    t = [x for x in range(1,(len(f1_score[i])+1))]\n",
    "    data = f1_score[i]\n",
    "#     fig, ax1 = plt.subplots()\n",
    "\n",
    "    color = 'tab:red'\n",
    "    plt.xlabel(\"Threshold(k)\")\n",
    "    plt.ylabel(\"F1-Score\")\n",
    "    plt.plot(t, data, color=color)\n",
    "    plt.tick_params(axis='y', labelcolor=color)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['file_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_new = [elem for elem in list(final['top_keywords'][1])]\n",
    "len(check_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(check_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_keywords[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(set(check_new).intersection(only_keywords[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Auslandsaufenthalt is  not being covered, because within text the word is 'auslandsaufenthalte'\n",
    "# or/and arbeitsbedingungen is not being covered because after lemmatization , it has changed to arbeitsbedingung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_new.index('auslandsaufenthalt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_recall_indices_new = []\n",
    "for i in range(len(recall_values)):\n",
    "    best_recall_indices_new.append(recall_values[i].index(max(recall_values[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_recall_indices_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_precision_indices_new = []\n",
    "for i in range(len(precison_values)):\n",
    "    best_precision_indices_new.append(precison_values[i].index(max(precison_values[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_precision_indices_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_recall_indices_element = []\n",
    "\n",
    "for i in range(len(best_recall_indices_new)):\n",
    "    Recall_values = recall_values[i]\n",
    "    best_recall_indices_element.append(Recall_values[best_recall_indices_new[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_recall_indices_element[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_precision_indices_element = []\n",
    "\n",
    "for i in range(len(best_precision_indices_new)):\n",
    "    Precision_values = precison_values[i]\n",
    "    best_precision_indices_element.append(Precision_values[best_precision_indices_new[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_precision_indices_element[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13, 8, 5, 11, 10, 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from germalemma import GermaLemma\n",
    "\n",
    "# lemmatizer = GermaLemma()\n",
    "\n",
    "# # passing the word and the POS tag (\"N\" for noun)\n",
    "# lemma = lemmatizer.find_lemma('arbeitsbedingungen', 'VERB')\n",
    "# print(lemma)\n",
    "# # -> lemma is \"Feinstaubbelastung\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "\n",
    "# doc = nlp(u'arbeitsbedingungen')\n",
    "\n",
    "# # show universal pos tags\n",
    "# print(' '.join('{word}/{tag}'.format(word=t.orth_, tag=t.pos_) for t in doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import spacy\n",
    "# nlp = spacy.load('de_core_news_lg')\n",
    "\n",
    "# mails=['arbeitsbedingungen']\n",
    "\n",
    "# mails_lemma = []\n",
    "\n",
    "# for mail in mails:\n",
    "#      doc = nlp(mail)\n",
    "#      result = ' '.join([x.lemma_ for x in doc]) \n",
    "#      mails_lemma.append(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
